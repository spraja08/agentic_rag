{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "387e6237-b616-40df-a697-6fc8f514b0f8",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This is a self-correcting RAG pattern that checks the retrieved contexts for relevancy and the generated answers for hallucinations.\\\n",
    "It is loosely based on this Self-RAG [paper](https://arxiv.org/abs/2310.11511)\n",
    "<img title=\"flow\"  src=\"resource/flow.png\">\n",
    "\n",
    "The LLM used in this is llama3:8b. The embedding model used is mxbai-embed-large (dim is 1024).\\\n",
    "Both are ran locally using ollama:\\\n",
    "a) Install ollama\\\n",
    "b) Pull llama3 and mxbai-embed-large (ollama pull...)\n",
    "\n",
    "Run the agentic_rag_index notebook before this to index and persist the context docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109ebc1-36a7-4ad7-8ab1-9bb9bbb3249b",
   "metadata": {},
   "source": [
    "### Build the Execution Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70cf04e3-75e2-43e3-b6ea-cbee45b51758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rspamzn/Documents/DevAx/Trainings/NN/agentic_rag/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---LOADING INDEX FROM PERSISTENNT STORE---\n",
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from agentic_rag_helper import Helper\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    context: List[str]\n",
    "    quality: str\n",
    "\n",
    "\n",
    "#retriever = retrieved_index.as_retriever()\n",
    "helper = Helper()\n",
    "helper.load_index(\"index\")\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"check_guardrails\", helper.guardtail_check) \n",
    "workflow.add_node(\"retrieve_context\", helper.retrieve_context) \n",
    "workflow.add_node(\"grade_documents\", helper.grade_chunks) \n",
    "workflow.add_node(\"generate\", helper.generate) \n",
    "workflow.add_node(\"grade_hallucination\", helper.grade_hallucination) \n",
    "\n",
    "workflow.set_entry_point(\"check_guardrails\")\n",
    "#workflow.add_edge(\"check_guardrails\", \"retrieve_context\")\n",
    "workflow.add_edge(\"retrieve_context\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_guardrails\",\n",
    "    helper.guardrail_decision,\n",
    "    {\n",
    "        \"stop\": END,\n",
    "        \"retrieve_context\": \"retrieve_context\",\n",
    "    }\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    helper.generation_decision,\n",
    "    {\n",
    "        \"stop\": END,\n",
    "        \"generate\": \"generate\",\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"generate\", \"grade_hallucination\")\n",
    "workflow.add_edge(\"grade_hallucination\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace34d27-3792-40a5-9769-1db55dc4d1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK FOR TOXICITY---\n",
      "---CLASSIFICASTION is NON_TOXIC--\n",
      "'Finished running: check_guardrails'\n",
      "---RETRIEVE---\n",
      "'Finished running: retrieve_context'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "Before that, I want to dive into making this article a little more contextual for developers. My current job scope is to augment developer productivity and in that scope, code generation using GenAI is an important weapon in any developers’ armoury. And we developers need to know when and where to apply this technology safely!\n",
      "\n",
      "Back to the 2 challenges of hallucination and lacking causality, the first problem is easy in the development domain. The generated code from the models can be easily fact checked — one just have to execute them. That is not hard. Most boiler plate codes have been working well in my tests. The hallucination flaws start to appear when you prompt for the not-so-common patterns (ex. generate a neural net algorithm for x inputs, y hidden nodes, z outputs using a certain activation function). In general application development, such requirements are rare and therefore, using the models to generate the codes do result in 50%+ productivity. The causality challenge is the worst one. In programming, often the decisions made at one part of the code will have multiple hops of connections to the rest. Imagine a state stored in a data member in an object that needs to be reset before a common algorithm in a method is executed. If the developer misses that (the cause) the program spits out bad results (the effect). If such a code is given to the generative models and asked for recommendations to fix it, they predictably struggle all the time. That is the hypothesis and we need to validate this to declare this as the challenging anti-pattern for applying generative models in programming.\n",
      "\n",
      "To prove the hypothesis, I took the case of coding a neural network itself. There is a poetic beauty in this scenario — to get a neural network (GenAI) to produce or fix another neural network. Self Replicating Machines, Wow ! To keep the problem simple, I coded the 2 hidden node network from a Josh Starmer video (btw, if you want to refresh the ML foundations, Josh has an amazingly intuitive guide).\n",
      "\n",
      "I purposefully avoided using the matrices and dot products so that I can create the problem scenario. This is the faulty code that will not converge after back propagation, despite any epoch size.\n",
      "\n",
      "I have coded a piece of langchain client that will invoke OpenAI APIs to get a recommended fix for this code. \n",
      "{'score': 'yes'}\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "Disclaimer : I could be wrong here and I had been wrong before :)\n",
      "{'score': 'no'}\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "'Finished running: grade_documents'\n",
      "---GENERATE---\n",
      "'Finished running: generate'\n",
      "---CHECK HALLUCINATIONS---\n",
      "{'score': 'yes'}\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "'Finished running: grade_hallucination'\n",
      "('{\"My current job scope is to augment developer productivity and in that '\n",
      " 'scope, code generation using GenAI is an important weapon in any developers’ '\n",
      " 'armoury.\" \\n'\n",
      " '\\n'\n",
      " '    \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '    \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '    \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '    \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '    \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '    \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '    \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '    \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '    \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '    \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n')\n"
     ]
    }
   ],
   "source": [
    "app = workflow.compile()\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"What is the author's current job scope?\"}\n",
    "#inputs = {\"question\": \"adjusting the heat using thermostats?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}\")\n",
    "if(len(value['context']) == 0):\n",
    "    pprint(\"No Relevant Chunks available in the Knowledgebase\")\n",
    "else:\n",
    "    pprint(value[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b5ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-rag",
   "language": "python",
   "name": "agentic-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
