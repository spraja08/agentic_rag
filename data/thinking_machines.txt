When I was an entrepreneur about a decade ago, I landed on a golden use case (or so I fantasised). It is to apply Natural Language Processing (NLP) to convert english statements into structured facts (Subject-Predicate-Object) that adhere to commonly-agreed Domain Ontologies. For example, “Singapore’s economic inflation is estimated at 4.5%” can be expressed as a triple with Singapore as the subject, 4.5 as the object and has_inflation as the predicate (semantic relatioship). All the Subjects, Objects and Predicates can be expressed as unique URIs so that the knowledge across millions of triples describing different aspects of the same subjects and objects can be stictched together. Such fact triples can be loaded onto Semantic Datastores (RDF or Property-Graph based) and queried, again by the same natural language processing capability. Even more so, these triples can be crunched by inferencing engines to derive semantically sound new knowledge and can reason these derivations in reverse too. The ultimate thinking machine! or so I fantasised.

Why this seemingly round-about approach to create a thinking machine as against the generative AI’s approach of predicting word-sequences? Why convert unstructured English to structured facts before processing it for an answer? And are there any other alternative approaches to creating thinking machines? Strangely, I have not heard much balanced critical opinions from contemporary computing historians or philosophers on this subject ever since LLMs were made public. It could also be that I am living under a certain other rock but all I hear is the whole industry monomaniacally adulating the genAI’s auto-regressive model, bullishly sidelining any reasonable criticisms. To correctly quantify the pleasantly surprising merits and some devastating demerits, we need to philosophically ponder on the humanity’s past quests on this matter and re-learn from it. With the very little that I have learned about this vast history, I will try and categorise the different schools of thoughts that existed and evolved this field. If you are a sincere seeker, you may stitch the story also, starting with Aristotle and traversing all the way through the history of first order logic, inferencing, mathematical induction, Descartes, George Boole, Gottlob Frege, David Hilbert etc.

Here is my version:

From 300BCE and all through the history, philosophers and mathematicians wanted a better alternative to the rhetorical approach to thinking and exploring truths. They audaciously believed that by using all the existing axioms and a set of inference rules, one can systematically work out all possible truths in the world. For this to happen, we needed a formal language using symbols and a grammar to represent axioms(ex. ∀x,Fruit(x)⇒Tasty(x)) and a set of inference rules to manipulate the symbols to create new axioms. In the early 20th century, several mathematicians and logicians, including Frege, Bertrand Russell, Whitehead, David Hilbert made significant contributions to this goal. Turing further took this dream to a possible promise of automating this process. Imagine a machine that endlessly manipulates symbols based on formal reasoning rules and derives all possible truths that you can trust! Here, we have our first school of thought and lets call this the “thinking = symbol manipulation” school. Apologies for shrinking this amazing history to a few lines for the sake of brevity but if you are an aficionado, we should connect :)

There was a brief period of time where the likes of Godel threw a spanner at the first school of thought. They did land on something that is correct with the incompleteness theorem. Basically, in a formal system that was proposed by the likes of Hilbert, every axiom (existing and derived) must be provable. That is the mandatory completeness requirement. With mathematical rigour, Godel proved that given any formal system, there would still be axioms that can not be provable. This shook the foundation of maths and we still feel the tremors caused by this. The eastern mystics have proposed the same theory centuries ago, calling it mayavada (the doctrine of illusion). For the sake of categorisation, let us call them the “thinking = (futile) illusion” school. Although this is profound, we don’t know how to handle this knowledge. So we conveniently parked it and went ahead on the legacy of Hilbert by inventing programming languages (like Prolog) and incessantly kept building symbol manipulating expert systems.

At least until the AI Winter.

The AI winter(s) did not happen suddenly. There were some radical voices that predicted it like that of Hubert Dreyfus. They theorised that the magical activities like object recognition are not symbol manipulation problems. These are perhaps pattern matching problems. So here is our third category and lets call them the “thinking = pattern sifting” school. Thanks to them, we learnt that we can’t solve a problem of recognising an object, say a chair, even if we write thousands of rules. (Although its a core component of intelligence, is recognition a sufficient capability to create thinking machines? Let us come back to this shortly)

While progressing in parallel on the natural language processing frontier, we have recently landed on another possible solution to thinking machines — the LLMs. This is a variation of the pattern sifting approach, applied to word patterns in languages. By mastering the language syntax patterns sourced from an unlimited corpus and by using them as references to generate eloquent statements, these models seem to simulate thinking. This is the claim of the fourth school and lets call them the “thinking = sequence transduction” school. This also would mean that people perhaps are nothing but evolved sequence models.

I am not going to dive into the second category for now. And the 3rd and the 4th are fundamentally similar in the approach. Therefore we can now reduce the categories into two to move forward — 1/symbol manipulation and; 2/sequence transduction using pattern matching. Now, it is important to understand the limitations of both these approaches to avoid rude shocks. In the first symbol manipulation approach, the trouble is the knowledge engineering bottleneck. Until the system assembles a critical mass of axioms, it would not be useful. Humanly seeding such an axiom base has folded in the past (refer to Tim Berners Lee’s earlier attempt at creating the Semantic Web). As a side story, I attempted to solve this problem using NLP to create the axioms as mentioned in the beginning of this article and could only achieve a limited success in a couple of narrow domains. The sequence transduction approach on the other hand lacks causality and it hallucinates. Both these are serious limitations. Those who know the math behind the LLMs understand how simple and dumb the structure of knowledge representation inside is and how challenging it would be to model chains of causes and effects. This is not a new problem. The linguists calls this Anaphora Resolution and it was recognised as the hard problem during the early NLP days itself.

Imagine a machine processing this statement — “I went to the movies last night but the tickets were sold out”. The semantic link missing in this statement is that one needs to buy tickets to get access to the movies. This is one degree of causality. Humans maintain and process massive chains of causalities in our reasoning. Relating statistical correlation scores of n-grams appearing in sentences using multi-dimensional matrices fails in representing such rich semantic relationships. Now, where it becomes problematic is when LLMs makes up the best guess using the closest patterns when it can’t find any tighter correlations. In strict mathematical terms, these are prediction errors but the industry chooses to call it as hallucination! When I was 6 years old, I was curious to find out how Nehru (the first Prime Minister of India) died. My uncle told me that Nehru got old, his eye-sight was so degraded that he fell in to a ditch while walking and died. Growing up in a god-forsaken rural part of the world, my uncle would have come across cases of old men going blind and dying in such unfortunate accidents. Although he did not have actual facts about Nehru’s death, he had great language skills to spin compelling narratives and those closely matching patterns. The combination of these resulted in this hallucination (actually a lie?). As a 6 year old, I fell for that eloquence and completely trusted him. I narrated that ‘fact’’ to my school teacher a few years down the line very confidently and the resultant awkward embarrassment stays with me forever. (Oh, Nehru died of heart failure while he was still a Prime Minister). Even a few days back, I was asking openAI to summarise a bunch of research papers for me. It gave me compelling answers that I fell for. When I read the actual papers later, I realised how completely wrong they were and felt like that same embarrassed and disappointed child.

Now, I am not a bitter road block prophet here. All I want is for us to have a a deep understanding of what problems can and can-not be solved, given a particular approach. Event though sequence transduction approach has its flaws, it is a surprising, highly critical and a very useful milestone. We will learn from it, use this as a stepping stone and synthesise a new paradigm. I have some clues of that synthesis which I will discuss in the end. Before that, I want to dive into making this article a little more contextual for developers. My current job scope is to augment developer productivity and in that scope, code generation using GenAI is an important weapon in any developers’ armoury. And we developers need to know when and where to apply this technology safely!

Back to the 2 challenges of hallucination and lacking causality, the first problem is easy in the development domain. The generated code from the models can be easily fact checked — one just have to execute them. That is not hard. Most boiler plate codes have been working well in my tests. The hallucination flaws start to appear when you prompt for the not-so-common patterns (ex. generate a neural net algorithm for x inputs, y hidden nodes, z outputs using a certain activation function). In general application development, such requirements are rare and therefore, using the models to generate the codes do result in 50%+ productivity. The causality challenge is the worst one. In programming, often the decisions made at one part of the code will have multiple hops of connections to the rest. Imagine a state stored in a data member in an object that needs to be reset before a common algorithm in a method is executed. If the developer misses that (the cause) the program spits out bad results (the effect). If such a code is given to the generative models and asked for recommendations to fix it, they predictably struggle all the time. That is the hypothesis and we need to validate this to declare this as the challenging anti-pattern for applying generative models in programming.

To prove the hypothesis, I took the case of coding a neural network itself. There is a poetic beauty in this scenario — to get a neural network (GenAI) to produce or fix another neural network. Self Replicating Machines, Wow ! To keep the problem simple, I coded the 2 hidden node network from a Josh Starmer video (btw, if you want to refresh the ML foundations, Josh has an amazingly intuitive guide).

I purposefully avoided using the matrices and dot products so that I can create the problem scenario. This is the faulty code that will not converge after back propagation, despite any epoch size.

I have coded a piece of langchain client that will invoke OpenAI APIs to get a recommended fix for this code. You may clone my github repo to execute this yourself. Set your OpenAI api key in OPENAI_API_KEY environment variable.

As expected, the program did not work with all the fixes recommended by the model. The trained weights and biases did not converge with the minimal loss value. Some of the fixes given were just redundant distractions. The model did pick up the error in derivate computation formulas for the weights as this pattern usually is directly present in the codes used during training. It is nice but I did expect that. But the model could not pick up the blunder of not resetting the predicted score variable from the previous epoch during forward propagation (cause-and-effect challenge). There are numerous manifestations of such cause-and-effect patterns that occur in general programming where the machines will struggle predictably. I will document all those moving forward so that we developers are aware of the pitfalls and we get a better experience from this otherwise amazing companion.

Now to the conclusion. As the old aphorism goes, all models are wrong; and some are useful. The sequence transduction approach suffers from the limitations of the rudimentary knowledge representation mechanism. The limitations of this will be amplified for the users soon (except in the creative use case scenarios) but the LLMs are nevertheless very useful and a great human achievement. The symbol manipulation approach comes from the deterministic mathematical world view of the 19th century. Unless it evolves to accommodate fuzziness and amasses the critical mass of the knowledge base, it will be inadequate too. But it will never hallucinate. There are also other tough philosophical and ethical chasms to cross for both the approaches. Thinking is not either one of pattern recognition or formal symbol manipulation. It is a synthesis of both. This is what I tried to apply in my attempt 10 years ago. (It was just the wrong timing as NLP capabilities were still primitive compared to today) Yejin Choi’s speech on this idea is super compelling too. The world of common sensical, hallucination-free thinking machines perhaps is not too far away.

Disclaimer : I could be wrong here and I had been wrong before :)